---
title: "Iris Project"
author: "Abigail Loy"
date: "October 14, 2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Machine Learning: Iris Dataset Classification!
Steps:
1. Bring the dataset into the R environment.
2. Split the data into Training and Testing sets.
3. Explore the  data.
4. Fit the training and testing sets to various models.
5. Summarize!

Packages Needed:
tidyverse
caret

-Bringing in the Iris Dataset and Taking a Look!-
  I'll be loading the dataset from directly within R, though you can also load it from an external source and it's most likely a good idea for you to do both.
```{r}
library(tidyverse)
library(caret)

#brings data into environment from within R
data(iris)

#gets help on dataset
help(iris)

#renames dataset
iris_dataset<-iris

#views dataset
View(iris_dataset)
```

```{r}
#prints first couple of rows
head(iris)
```

-Train - Test - Split!-
  Here we'll be splitting the data into a training and testing set.
  The training data set is to help us understand the data, select the appropriate model and determine model parameters. For this classification problem, we'll train the model using the classfication error rate, which is the percentage of incorrectly/correctly classified instances.
  The testing set is for testing our model's preformance when classifying new data. The model performance(error rate) on this new data will be a more realistic estimate of the model fit in the real world.
  
```{r}
#creates a partition (80% training, 20% testing)
index <- createDataPartition(iris_dataset$Species, p=0.80, list=FALSE)

#selects 20% of the data for testing
testset <- iris_dataset[-index,]

#selects 80% of data to train models
trainset <- iris_dataset[index,]
```

-Exploratory Data Analysis-
  Exploring the data is to get us more familiar with the dataset and making sure we understand what the rows and columns are and what the dataset is supposed to be illustrating and how we can use it to build visuals.
```{r}
#gets dimensions of the dataset
dim(trainset)
```

```{r}
#gets structure of the dataset
str(trainset)
```

```{r}
#gets summary of the dataset
summary(trainset)
```

```{r}
#gets levels of the prediction column
levels(trainset$Species)
```

-Visualizing!-
  Now we need to visualize the data! Visualizing the data helps with understanding it's various attributes and how they relate to each other. For our plots, we're going to be making some histograms, box plots, and scatter plots. We'll start with simplistic plots and then add some flare. lastly, we'll try faceting, making multiple charts in one plot.
```{r}
#histogram: frequency of Sepal Width
hist(trainset$Sepal.Width)
```

```{r}
#box plot: how distribution varies by class of flower
par(mfrow=c(1,4))
  for(i in 1:4) {
  boxplot(trainset[,i], main=names(trainset)[i])
}
```
  Now we'll be using the ggplot package to plot some more complicated and percise graphs and with colors!
```{r}
#creates base scatter plot
base <- ggplot(data=trainset, aes(x = Petal.Length, y = Petal.Width))
print(base)
```

```{r}
#scatter plot: distribution of species based on Petal Length and Width
scatter <-base + 
    geom_point(aes(color=Species, shape=Species)) +
    xlab("Petal Length") +
    ylab("Petal Width") +
    ggtitle("Petal Length-Width")+
    geom_smooth(method="lm")
print(scatter)
```

```{r}
#box plot: distribution of Species based on Sepal Length
box <- ggplot(data=trainset, aes(x=Species, y=Sepal.Length)) +
    geom_boxplot(aes(fill=Species)) + 
    ylab("Sepal Length") +
    ggtitle("Iris Boxplot") +
    stat_summary(fun.y=mean, geom="point", shape=5, size=4) 
print(box)
```

```{r}
#histogram: frequency of species of flower based on Sepal Width
histo <- ggplot(data=iris, aes(x=Sepal.Width)) +
    geom_histogram(binwidth=0.2, color="black", aes(fill=Species)) + 
    xlab("Sepal Width") +  
    ylab("Frequency") + 
    ggtitle("Histogram of Sepal Width")
print(histo)
```

```{r}
#faceting: distribution of all species' Sepal Lengths and Widths
face <- ggplot(data=trainset, aes(Sepal.Length, y=Sepal.Width, color=Species))+
    geom_point(aes(shape=Species), size=1.5) + 
    geom_smooth(method="lm") +
    xlab("Sepal Length") +
    ylab("Sepal Width") +
    ggtitle("Faceting") +
    facet_grid(. ~ Species)
print(face)
```

-Machine Learning Models!-
  Finally we're ready to fit our dataset to some models! We'll be building a few models and use the 'trainset' to try and narrow down which one is the best model for predicting the 'testset,' and then we'll try to measure how it will preform in the real world.
  
  Decision Tree Classifier
  This kind of model classifies observations by sorting them down the 'tree' from the root node to the leaf node, which provides the classification for the observation. Each node specifies a test on a particular attribute and each branch from that node represents one of the possible values for that test.
```{r}
tree <- rpart(yesno ~., data = trainset)
```

```{r}
plot(rpart$finalModel)
text(rpart$finalModel)
```

```{r}
#predictions on 'trainset'
pred<-table(predict(object = model.rpart$finalModel,newdata = trainset[,1:4],type="class"))
pred
```

```{r}
#checks accuracy using a confusion matrix by comparing predictions to actual classifications
confusionMatrix(predict(object = model.rpart$finalModel,newdata = trainset[,1:4],type="class"),trainset$Species)
```

```{r}
#checks accuracy on 'testset'
pred_test<-predict(object = model.rpart$finalModel,
                   newdata = testset[,1:4],
                   type="class")
confusionMatrix(pred_test,testset$Species)
```

  Linear Discriminant Analysis
  This classifier is most commonly used as a dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting and also reduce computational costs.
```{r}
set.seed(1000)

#fits model
model.lda<-train(x = trainset[,1:4],y = trainset[,5], method = "lda",metric = "Accuracy")

#prints model
print(model.lda)
```

```{r}
#verifies accuracy on 'trainset'
pred<-predict(object = model.lda,newdata = trainset[,1:4])
confusionMatrix(pred,trainset$Species)
```

```{r}
#verifies accuracy on 'testset'
pred_test<-predict(object = model.lda,newdata = testset[,1:4])
confusionMatrix(pred_test,testset$Species)
```

  Random Forest Algorithm
  This kind of classifier creates a model that averages the predictions from many other such classification trees. This class of algorithims uses different attributes for growing each tree.
```{r}
print(model.rf)
```

```{r}
confusionMatrix(pred,trainset$Species)
```

```{r}
#verifies accuracy on 'testset'
pred_test<-predict(object = model.rf$finalModel,newdata = testset[,1:4],type="class")
confusionMatrix(pred_test,testset$Species)
```

  Gradient Boosting Method
  This classifier uses a technique called 'boosting' where we still grow decision classification trees, but each successive tree is grown with an intent to correctly classify the missclassified data from the previous tree.
```{r}
print(model.gbm)
```

```{r}
#verifies accuracy on 'trainset'
pred<-predict(object = model.gbm,newdata = trainset[,1:4])
confusionMatrix(pred,trainset$Species)
```

```{r}
confusionMatrix(pred_test,testset$Species)
```

  K Means Clustering Model
  This model is a type of unsurpervised learning that uses clustering. It's an exploratory data analysis technique used for identifying groups in the data, wih aach group containing observations with similar profiles according to specific criteria. Similarity between observations is defined using some inter-observation distance measures including Euclidean and correlation-based distance measures.
```{r}
#sets seed to ensure reproduceability
set.seed(20)
```

```{r}
irisCluster <- kmeans(iris[, 1:4], centers = 3, nstart = 20)
irisCluster
```

```{r}
#checks classification accuracy
table(irisCluster$cluster, iris$Species)
```

```{r}
plot(iris[c("Sepal.Length", "Sepal.Width")], col=irisCluster$cluster)
points(irisCluster$centers[,c("Sepal.Length", "Sepal.Width")], col=1:3, pch=8, cex=2)
```

Summarizing Accuracy
```{r}
#summarizes accuracy of models
results <- resamples(list(TREE=model.rpart, RandomForest=model.rf, GBM=model.gbm, LDA=model.lda))
summary(results)
```

```{r}
dotplot(results)
```

